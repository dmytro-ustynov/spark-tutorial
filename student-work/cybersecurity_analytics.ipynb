{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cybersecurity Analytics with Apache Spark Streaming\n",
    "\n",
    "This notebook demonstrates real-time cybersecurity threat detection using Apache Spark Structured Streaming.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Process real-time security event streams with Spark\n",
    "- Detect brute force attacks using time-based aggregations\n",
    "- Identify DDoS attacks through traffic pattern analysis\n",
    "- Handle late-arriving data with watermarking\n",
    "- Store analytical results for further investigation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure:\n",
    "1. Docker environment is running: `docker-compose up -d`\n",
    "2. All services are healthy: `./lab-control.sh status`\n",
    "3. Events are flowing: `python verify_events.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, let's set up our environment and import necessary libraries."
   ]
  },  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import socket\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üìö Libraries imported successfully\")\n",
    "print(\"üéØ Ready to start cybersecurity analytics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Session Configuration\n",
    "\n",
    "Configure Spark with the necessary packages for Kafka integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_environment():\n",
    "    \"\"\"Detect if we're running in container or local environment\"\"\"\n",
    "    try:\n",
    "        socket.gethostbyname('kafka')\n",
    "        return \"container\"\n",
    "    except socket.gaierror:\n",
    "        return \"local\"\n",
    "\n",
    "# Detect environment\n",
    "environment = detect_environment()\n",
    "print(f\"üîç Environment detected: {environment}\")\n",
    "\n",
    "# Set Kafka packages for Spark\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell\"\n",
    "\n",
    "# Create unique checkpoint directory\n",
    "checkpoint_dir = f\"/tmp/spark-cybersec-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "print(f\"üìÅ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CybersecurityAnalytics\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", checkpoint_dir) \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduce log verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark session initialized successfully\")\n",
    "print(f\"üåü Spark version: {spark.version}\")\n",
    "print(f\"üîó Spark UI available at: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Event Schema\n",
    "\n",
    "Define the structure of our security events for proper parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for security events\n",
    "event_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"source_ip\", StringType(), True),\n",
    "    StructField(\"destination_ip\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"result\", StringType(), True),\n",
    "    StructField(\"protocol\", StringType(), True),\n",
    "    StructField(\"port\", IntegerType(), True),\n",
    "    StructField(\"severity\", StringType(), True),\n",
    "    StructField(\"failure_reason\", StringType(), True),\n",
    "    StructField(\"geo_location\", StructType([\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "print(\"üìã Event schema defined\")\n",
    "print(\"üîç Schema includes: timestamp, IPs, authentication data, geo location\")"
   ]
  },  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kafka Connection Setup\n",
    "\n",
    "Establish connection to Kafka to read streaming security events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kafka_server(environment):\n",
    "    \"\"\"Get the correct Kafka server based on environment\"\"\"\n",
    "    if environment == \"container\":\n",
    "        return \"kafka:29092\"\n",
    "    else:\n",
    "        return \"localhost:9092\"\n",
    "\n",
    "kafka_server = get_kafka_server(environment)\n",
    "kafka_topic = \"security-events\"\n",
    "\n",
    "print(f\"üîó Connecting to Kafka server: {kafka_server}\")\n",
    "print(f\"üì° Reading from topic: {kafka_topic}\")\n",
    "\n",
    "# Create Kafka streaming DataFrame\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"‚úÖ Kafka streaming DataFrame created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse JSON Events\n",
    "\n",
    "Parse the JSON events from Kafka and convert timestamps for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON events and convert to proper DataFrame\n",
    "events_df = kafka_df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"event\")) \\\n",
    "    .select(\"event.*\") \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .filter(col(\"timestamp\").isNotNull())\n",
    "\n",
    "print(\"‚úÖ JSON parsing configured\")\n",
    "print(\"üïí Timestamp conversion applied\")\n",
    "print(\"üßπ Null timestamp filtering enabled\")"
   ]
  },  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Event Monitoring\n",
    "\n",
    "Let's first verify that events are flowing correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Starting basic event monitoring...\")\n",
    "print(\"üí° This will show live events for 30 seconds\")\n",
    "print(\"üìä You should see authentication attempts, network connections, etc.\")\n",
    "\n",
    "# Simple event monitoring query\n",
    "basic_monitor = events_df \\\n",
    "    .select(\"timestamp\", \"event_type\", \"source_ip\", \"result\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Monitor for 30 seconds\n",
    "time.sleep(30)\n",
    "basic_monitor.stop()\n",
    "\n",
    "print(\"\\n‚úÖ Basic monitoring complete\")\n",
    "print(\"üéØ Ready to implement threat detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Brute Force Attack Detection\n",
    "\n",
    "Now let's implement our first threat detection: brute force attacks.\n",
    "\n",
    "**Detection Logic:**\n",
    "- Monitor failed authentication attempts\n",
    "- Count failures per source IP in time windows\n",
    "- Alert when threshold is exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_brute_force_attacks(events_df, threshold=5):\n",
    "    \"\"\"\n",
    "    Detect brute force attacks by monitoring failed authentication attempts\n",
    "    \n",
    "    Args:\n",
    "        events_df: Streaming DataFrame of security events\n",
    "        threshold: Minimum failed attempts to trigger alert (default: 5)\n",
    "    \"\"\"\n",
    "    \n",
    "    brute_force_alerts = events_df \\\n",
    "        .filter(col(\"event_type\") == \"authentication\") \\\n",
    "        .filter(col(\"result\") == \"failure\") \\\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"2 minutes\", \"30 seconds\"),\n",
    "            col(\"source_ip\"),\n",
    "            col(\"destination_ip\")\n",
    "        ) \\\n",
    "        .count() \\\n",
    "        .filter(col(\"count\") >= threshold) \\\n",
    "        .select(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"source_ip\"),\n",
    "            col(\"destination_ip\").alias(\"target_ip\"),\n",
    "            col(\"count\").alias(\"failed_attempts\"),\n",
    "            lit(\"üö® BRUTE_FORCE_DETECTED\").alias(\"alert_type\"),\n",
    "            current_timestamp().alias(\"detected_at\")\n",
    "        )\n",
    "    \n",
    "    return brute_force_alerts\n",
    "\n",
    "# Create brute force detection\n",
    "print(\"üîç Setting up brute force detection...\")\n",
    "print(\"Threshold: ‚â•5 failed attempts in 2-minute windows\")\n",
    "print(\"Window slides every 30 seconds for real-time detection\")\n",
    "\n",
    "brute_force_alerts = detect_brute_force_attacks(events_df, threshold=5)\n",
    "\n",
    "print(\"‚úÖ Brute force detection configured\")\n",
    "print(\"üéØ Start an attack to see alerts: ./lab-control.sh attack-bf\")"
   ]
  },  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Brute Force Detection\n",
    "\n",
    "Let's start the detection and monitor for alerts. Make sure to start an attack in another terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üö® Starting brute force detection monitor...\")\n",
    "print(\"üí° In another terminal, run: ./lab-control.sh attack-bf\")\n",
    "print(\"‚è±Ô∏è  Expected detection time: 2-3 minutes after attack starts\")\n",
    "print(\"\\nüîç Watching for alerts (monitoring for 2 minutes)...\")\n",
    "\n",
    "# Start the brute force detection query\n",
    "brute_force_query = brute_force_alerts \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Monitor for 2 minutes\n",
    "time.sleep(120)\n",
    "brute_force_query.stop()\n",
    "\n",
    "print(\"\\n‚úÖ Brute force detection monitoring complete\")\n",
    "print(\"üìä Analysis: Each alert shows source_ip, target_ip, and failed_attempts count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. DDoS Detection\n",
    "\n",
    "Let's also implement DDoS detection by monitoring network connection patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ddos_attacks(events_df, threshold=100):\n",
    "    \"\"\"\n",
    "    Detect DDoS attacks by monitoring high-volume traffic to single targets\n",
    "    \n",
    "    Args:\n",
    "        events_df: Streaming DataFrame of security events\n",
    "        threshold: Minimum requests per minute to trigger alert (default: 100)\n",
    "    \"\"\"\n",
    "    \n",
    "    ddos_alerts = events_df \\\n",
    "        .filter(col(\"event_type\") == \"network_connection\") \\\n",
    "        .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"1 minute\", \"30 seconds\"),\n",
    "            col(\"destination_ip\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"request_count\"),\n",
    "            approx_count_distinct(\"source_ip\").alias(\"unique_sources\")\n",
    "        ) \\\n",
    "        .filter(col(\"request_count\") >= threshold) \\\n",
    "        .select(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"destination_ip\").alias(\"target_ip\"),\n",
    "            col(\"request_count\"),\n",
    "            col(\"unique_sources\"),\n",
    "            lit(\"üö® DDOS_DETECTED\").alias(\"alert_type\"),\n",
    "            current_timestamp().alias(\"detected_at\")\n",
    "        )\n",
    "    \n",
    "    return ddos_alerts\n",
    "\n",
    "# Create DDoS detection\n",
    "print(\"üîç Setting up DDoS detection...\")\n",
    "print(\"Threshold: ‚â•100 requests per minute to single target\")\n",
    "print(\"Using approx_count_distinct for streaming compatibility\")\n",
    "\n",
    "ddos_alerts = detect_ddos_attacks(events_df, threshold=100)\n",
    "\n",
    "print(\"‚úÖ DDoS detection configured\")\n",
    "print(\"üéØ Start DDoS attack: ./lab-control.sh attack-ddos\")"
   ]
  },  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Combined Monitoring Dashboard\n",
    "\n",
    "Let's create a combined monitoring setup that shows both brute force and DDoS detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_combined_monitoring(duration_minutes=3):\n",
    "    \"\"\"\n",
    "    Start combined monitoring for both brute force and DDoS attacks\n",
    "    \n",
    "    Args:\n",
    "        duration_minutes: How long to monitor (default: 3 minutes)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üö® Starting Combined Security Monitoring Dashboard\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí° In separate terminals, you can run:\")\n",
    "    print(\"   ‚Ä¢ ./lab-control.sh attack-bf    (brute force)\")\n",
    "    print(\"   ‚Ä¢ ./lab-control.sh attack-ddos  (DDoS)\")\n",
    "    print(f\"\\n‚è±Ô∏è  Monitoring for {duration_minutes} minutes...\")\n",
    "    print(\"üîç Alerts will appear every 30 seconds\\n\")\n",
    "    \n",
    "    # Start brute force monitoring\n",
    "    bf_query = brute_force_alerts \\\n",
    "        .writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .option(\"numRows\", 5) \\\n",
    "        .trigger(processingTime=\"30 seconds\") \\\n",
    "        .queryName(\"BruteForceDetection\") \\\n",
    "        .start()\n",
    "    \n",
    "    # Start DDoS monitoring\n",
    "    ddos_query = ddos_alerts \\\n",
    "        .writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .option(\"numRows\", 5) \\\n",
    "        .trigger(processingTime=\"30 seconds\") \\\n",
    "        .queryName(\"DDoSDetection\") \\\n",
    "        .start()\n",
    "    \n",
    "    # Monitor for specified duration\n",
    "    duration_seconds = duration_minutes * 60\n",
    "    time.sleep(duration_seconds)\n",
    "    \n",
    "    # Stop both queries\n",
    "    bf_query.stop()\n",
    "    ddos_query.stop()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Combined monitoring complete\")\n",
    "    print(\"üìä Summary of what you should have observed:\")\n",
    "    print(\"   ‚Ä¢ Brute force alerts: source_ip with high failed_attempts\")\n",
    "    print(\"   ‚Ä¢ DDoS alerts: target_ip with high request_count\")\n",
    "    print(\"   ‚Ä¢ Real-time detection within 2-3 minutes of attack start\")\n",
    "\n",
    "# Start the combined monitoring\n",
    "start_combined_monitoring(duration_minutes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Experiment with Thresholds\n",
    "\n",
    "Let's experiment with different detection thresholds to understand their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_thresholds():\n",
    "    \"\"\"Experiment with different detection thresholds\"\"\"\n",
    "    \n",
    "    print(\"üß™ Threshold Experimentation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Different thresholds to test\n",
    "    thresholds = [3, 5, 10, 20]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nüîç Testing threshold: {threshold} failed attempts\")\n",
    "        \n",
    "        # Create detection with this threshold\n",
    "        test_alerts = detect_brute_force_attacks(events_df, threshold=threshold)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Lower threshold = More sensitive (more alerts)\")\n",
    "        print(f\"   ‚Ä¢ Higher threshold = Less sensitive (fewer alerts)\")\n",
    "        print(f\"   ‚Ä¢ Threshold {threshold}: {'Sensitive' if threshold <= 5 else 'Conservative'}\")\n",
    "    \n",
    "    print(\"\\nüìä Threshold Selection Guidelines:\")\n",
    "    print(\"   ‚Ä¢ threshold=3: Very sensitive, may have false positives\")\n",
    "    print(\"   ‚Ä¢ threshold=5: Balanced (recommended for lab)\")\n",
    "    print(\"   ‚Ä¢ threshold=10: Conservative, fewer false positives\")\n",
    "    print(\"   ‚Ä¢ threshold=20: Very conservative, may miss attacks\")\n",
    "    \n",
    "    print(\"\\nüí° Production Recommendation:\")\n",
    "    print(\"   Start with threshold=5, then adjust based on your environment\")\n",
    "\n",
    "# Run the threshold experiment\n",
    "experiment_with_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Database Integration (Advanced)\n",
    "\n",
    "For production systems, you'll want to store alerts in a database. Here's how to set that up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_alerts_to_database(alerts_df, table_name):\n",
    "    \"\"\"\n",
    "    Write streaming alerts to PostgreSQL database\n",
    "    \n",
    "    Note: This requires foreachBatch for streaming database writes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Database configuration\n",
    "    db_properties = {\n",
    "        \"user\": \"spark_user\",\n",
    "        \"password\": \"spark_password\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"url\": \"jdbc:postgresql://postgres:5432/security_analytics\"\n",
    "    }\n",
    "    \n",
    "    def write_batch_to_postgres(batch_df, batch_id):\n",
    "        \"\"\"Write each batch to PostgreSQL\"\"\"\n",
    "        if batch_df.count() > 0:\n",
    "            print(f\"üìù Writing {batch_df.count()} alerts to database (batch {batch_id})\")\n",
    "            batch_df.write \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "                .option(\"url\", db_properties[\"url\"]) \\\n",
    "                .option(\"user\", db_properties[\"user\"]) \\\n",
    "                .option(\"password\", db_properties[\"password\"]) \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .save()\n",
    "    \n",
    "    # Create streaming query with database writes\n",
    "    query = alerts_df \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(write_batch_to_postgres) \\\n",
    "        .trigger(processingTime=\"30 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    return query\n",
    "\n",
    "print(\"üíæ Database Integration Setup\")\n",
    "print(\"=\" * 30)\n",
    "print(\"üéØ Next Steps for Students:\")\n",
    "print(\"1. Uncomment the code below to enable database writes\")\n",
    "print(\"2. Run: docker-compose exec postgres psql -U spark_user -d security_analytics\")\n",
    "print(\"3. Query: SELECT * FROM brute_force_alerts ORDER BY detected_at DESC;\")\n",
    "print(\"\\nüí° This demonstrates production-ready alert storage\")\n",
    "\n",
    "# Uncomment these lines to enable database writes:\n",
    "# db_query = write_alerts_to_database(brute_force_alerts, \"brute_force_alerts\")\n",
    "# time.sleep(60)  # Run for 1 minute\n",
    "# db_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully implemented real-time cybersecurity analytics with Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary():\n",
    "    \"\"\"Display learning summary and next steps\"\"\"\n",
    "    \n",
    "    print(\"üéì Learning Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\n‚úÖ What You've Accomplished:\")\n",
    "    print(\"   ‚Ä¢ Set up Spark Structured Streaming with Kafka\")\n",
    "    print(\"   ‚Ä¢ Implemented real-time brute force detection\")\n",
    "    print(\"   ‚Ä¢ Created DDoS attack monitoring\")\n",
    "    print(\"   ‚Ä¢ Used windowed aggregations and watermarking\")\n",
    "    print(\"   ‚Ä¢ Experimented with detection thresholds\")\n",
    "    print(\"   ‚Ä¢ Learned about database integration patterns\")\n",
    "    \n",
    "    print(\"\\nüìä Key Concepts Learned:\")\n",
    "    print(\"   ‚Ä¢ Streaming DataFrames and transformations\")\n",
    "    print(\"   ‚Ä¢ Window functions for time-based analysis\")\n",
    "    print(\"   ‚Ä¢ Watermarking for handling late data\")\n",
    "    print(\"   ‚Ä¢ Approximate aggregations (approx_count_distinct)\")\n",
    "    print(\"   ‚Ä¢ foreachBatch for custom output handling\")\n",
    "    \n",
    "    print(\"\\nüöÄ Next Steps & Advanced Topics:\")\n",
    "    print(\"   1. Implement geographic anomaly detection\")\n",
    "    print(\"   2. Add machine learning for anomaly detection\")\n",
    "    print(\"   3. Create alerting integrations (Slack, email)\")\n",
    "    print(\"   4. Build a real-time security dashboard\")\n",
    "    print(\"   5. Optimize performance for high-volume streams\")\n",
    "    print(\"   6. Implement multi-stage threat scoring\")\n",
    "    \n",
    "    print(\"\\nüí° Production Considerations:\")\n",
    "    print(\"   ‚Ä¢ Monitor streaming query health and performance\")\n",
    "    print(\"   ‚Ä¢ Implement proper error handling and recovery\")\n",
    "    print(\"   ‚Ä¢ Set up checkpointing for fault tolerance\")\n",
    "    print(\"   ‚Ä¢ Configure resource allocation and scaling\")\n",
    "    print(\"   ‚Ä¢ Establish alert tuning and false positive reduction\")\n",
    "    \n",
    "    print(\"\\nüîó Useful Resources:\")\n",
    "    print(\"   ‚Ä¢ Spark Streaming Guide: https://spark.apache.org/streaming/\")\n",
    "    print(\"   ‚Ä¢ Kafka Integration: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\")\n",
    "    print(\"   ‚Ä¢ Security Analytics Patterns: Research SIEM and SOC practices\")\n",
    "\n",
    "# Display the summary\n",
    "show_summary()\n",
    "\n",
    "# Stop Spark session\n",
    "print(\"\\nüõë Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Session ended. Great work on completing the cybersecurity analytics lab!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
